{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import NuSVR # nu-SVR, implement from libsvm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "# different regressor test\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import pandas as pd # process txt\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# 645x312的输入，312x1024的矩阵，变换为645x1024的输出\n",
    "# 43 * 15\n",
    "import torch \n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard_split: 645 72\n",
      "proposed_split: 645 72\n"
     ]
    }
   ],
   "source": [
    "folder = \"G:/dataset/sun/\"\n",
    "\n",
    "standard_path = \"G:/dataset/standard_split/SUN/\"\n",
    "proposed_path = \"G:/dataset/proposed_split/SUN/\"\n",
    "\n",
    "\n",
    "cls_to_idx = {}\n",
    "cnt = 0 \n",
    "with open(standard_path + \"allclasses.txt\", \"r\", encoding='utf-8') as f: # TBD\n",
    "     for row in f.readlines():\n",
    "         row = row.rstrip()\n",
    "         cls_to_idx[row] = cnt\n",
    "         cnt = cnt + 1\n",
    "\n",
    "sstrain, sstest = [], []\n",
    "pstrain, pstest = [], []\n",
    "\n",
    "# Standard Split \n",
    "with open(standard_path + \"trainvalclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        sstrain.append(cls_to_idx[row])\n",
    "\n",
    "with open(standard_path + \"testclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        sstest.append(cls_to_idx[row])\n",
    "\n",
    "print(\"standard_split:\", len(sstrain), len(sstest))\n",
    "# transform List(str) -> List(int)\n",
    "\n",
    "# Proposed Split\n",
    "with open(proposed_path + \"trainvalclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        pstrain.append(cls_to_idx[row])\n",
    "\n",
    "with open(proposed_path + \"testclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        pstest.append(cls_to_idx[row])\n",
    "\n",
    "print(\"proposed_split:\", len(pstrain), len(pstest))\n",
    "\n",
    "# Random Train & Test Class Split\n",
    "\n",
    "train_class, test_class = [], [] \n",
    "X_class = list(range(717)) # label start from 0, you can adjust it\n",
    "train_class, test_class = train_test_split(X_class, test_size=0.2)\n",
    "\n",
    "# len(train_class), len(test_class) == (573, 144)\n",
    "\n",
    "train_class, test_class = sstrain, sstest\n",
    "train_class, test_class = pstrain, pstest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12900, 2048) (12900,)\n",
      "(1440, 2048) (1440,)\n"
     ]
    }
   ],
   "source": [
    "# semantic embedding of SUN, 717 scenes, 102 attributes\n",
    "# fork s.e. from essay \"evaluation\"\n",
    "#se = folder + \"sun_se_continuous.txt\"\n",
    "\n",
    "#semat = att['att'].T # transpose\n",
    "#semat = np.loadtxt(se, delimiter=\" \", encoding='utf-8')\n",
    "\n",
    "\n",
    "class sunRead:\n",
    "    def __init__(self, p, train_split):\n",
    "        \"\"\"\n",
    "        p: 数据集存放路径\n",
    "        train_split: 给出训练集\n",
    "        \"\"\"\n",
    "        X_class = list(range(717))\n",
    "        train_class = train_split\n",
    "        test_class = list(filter(lambda i: i not in train_class, X_class))\n",
    "        self.path = p\n",
    "        # labels\n",
    "        yp = self.path + \"custom-sun-labels.txt\"\n",
    "        y = np.loadtxt(yp, delimiter=\" \", encoding='utf-8')\n",
    "        # visual features 2048 dimensions\n",
    "        xp = self.path + \"sun-features.txt\"\n",
    "        x = np.loadtxt(xp, delimiter=\" \", encoding='utf-8')\n",
    "        i1 = np.isin(y, train_class)\n",
    "        i2 = np.isin(y, test_class)\n",
    "        self.X_train, self.X_test = x[i1], x[i2]\n",
    "        self.y_train, self.y_test = y[i1], y[i2]\n",
    "    def train_data(self):\n",
    "        return self.X_train, self.y_train\n",
    "    def test_data(self):\n",
    "        return self.X_test, self.y_test\n",
    "    def test(self): # 11788 all, add them to test\n",
    "        print(self.X_train.shape, self.y_train.shape)\n",
    "        print(self.X_test.shape, self.y_test.shape)\n",
    "\n",
    "sunReader = sunRead(folder, train_class)\n",
    "sunReader.test()\n",
    "# custom\n",
    "# standard 12900 + 1440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = loadmat(standard_path + \"att_splits.mat\")\n",
    "semat = att['att_original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102, 717), (717, 102))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadmat(standard_path + \"att_splits.mat\")['att'].shape, loadmat(standard_path + \"att_splits.mat\")['att_original'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12900, 500), (12900,), (1440, 500), (1440,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = sunReader.train_data()\n",
    "X_test, y_test = sunReader.test_data()\n",
    "#X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "# hyper_parameter\n",
    "pca_d = 500\n",
    "\n",
    "exemPCA = decomposition.PCA(n_components=pca_d)\n",
    "exemPCA.fit(X_train)\n",
    "X_train = exemPCA.transform(X_train)\n",
    "X_test = exemPCA.transform(X_test)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group up PCA projections\n",
    "\n",
    "exem_train_group = defaultdict(list)\n",
    "\n",
    "for c in train_class:\n",
    "    exem_train_group[c] = []\n",
    "        \n",
    "for x, y in zip(X_train, y_train):\n",
    "    exem_train_group[y].append(x)\n",
    "\n",
    "\n",
    "# Average\n",
    "exem_train = {}\n",
    "std_train = {}\n",
    "k = 0\n",
    "\n",
    "for item in exem_train_group.items():\n",
    "    y, ary = item\n",
    "    exem_train[y] = np.mean(ary, axis=0) # Key Sentence\n",
    "    std_train[y] = np.std(ary, axis=0)\n",
    "\n",
    "del exem_train_group\n",
    "\n",
    "# class to index, the same\n",
    "a_c_train, a_c_test = semat[train_class], semat[test_class]\n",
    "v_c_train = [exem_train[i] for i in train_class] # dict uses classes to index\n",
    "sd_train = [std_train[i] for i in train_class]\n",
    "sd_train = np.mean(sd_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((645, 102), (645, 500), (500,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a_c_train).shape, np.array(v_c_train).shape, np.array(sd_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2步 \n",
    "### Training\n",
    "\n",
    "用MLP训练，\n",
    "\n",
    "\n",
    "X为语义表示，是固定的a_c_train矩阵，维度为645x102转换为645x1024\n",
    "\n",
    "$$\\psi (a_c) \\approx v_c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "x, y = torch.tensor(np.array(a_c_train), dtype=torch.float), torch.tensor(np.array(v_c_train), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 10374.3837890625\n",
      "599 5953.52197265625\n",
      "1099 4984.41650390625\n",
      "1599 4374.96240234375\n",
      "2099 3931.044921875\n",
      "2599 3583.190673828125\n"
     ]
    }
   ],
   "source": [
    "# Torch Training\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 43, 102, 300, pca_d\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "#x, y = torch.randn(N, D_in), torch.randn(N, D_out)\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    #torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(3000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    for i in range(len(x)//N):\n",
    "        inputs = x[i*N:i*N + N]\n",
    "        labels = y[i*N:i*N + N]\n",
    "        y_pred = model(inputs)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "    if t % 500 == 99:\n",
    "        print(t, loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1NN:0.6125 \n",
      "1NNs:0.6270833333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_y = model(torch.tensor(a_c_test, dtype=torch.float))\n",
    "test_y.shape\n",
    "v_c_test = test_y.detach().numpy()\n",
    "\n",
    "\n",
    "v_c_test.shape, len(v_c_train), len(v_c_train[0])\n",
    "\n",
    "exem_X, exem_y = [], []\n",
    "\n",
    "for i, c in enumerate(test_class):\n",
    "    exem_X.append(v_c_test[i])\n",
    "    exem_y.append(c)\n",
    "    \n",
    "# Add this part to become GZSL\n",
    "#for i, c in enumerate(train_class):\n",
    "#    exem_X.append(v_c_train[i])\n",
    "#    exem_y.append(c)\n",
    "\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(exem_X, exem_y)\n",
    "print(\"1NN:{} \".format(neigh.score(X_test, y_test)))\n",
    "\n",
    "sneigh = KNeighborsClassifier(n_neighbors=1, metric='seuclidean', \n",
    "                    metric_params={'V':sd_train})\n",
    "sneigh.fit(exem_X, exem_y)\n",
    "print(\"1NNs:{}\".format(sneigh.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Log of Training\n",
    "\n",
    "```\n",
    "99 10338.1181640625\n",
    "399 6707.013671875\n",
    "699 5677.1259765625\n",
    "999 5096.4580078125\n",
    "1299 4654.57080078125\n",
    "1599 4294.96728515625\n",
    "1899 4000.2236328125\n",
    "2199 3748.1884765625\n",
    "2499 3532.768310546875\n",
    "2799 3342.576904296875\n",
    "\n",
    "1NN:0.6090277777777777 \n",
    "1NNs:0.6277777777777778\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout + ReLU\n",
    "\n",
    "```\n",
    "99 11005.8896484375\n",
    "799 6164.46337890625\n",
    "1499 5249.7626953125\n",
    "2199 4515.26025390625\n",
    "2899 4208.16455078125\n",
    "3599 4074.322509765625\n",
    "4299 3824.184814453125\n",
    "4999 3620.673095703125\n",
    "5699 3504.84814453125\n",
    "6399 3528.818115234375\n",
    "\n",
    "1NN:0.5715277777777777 \n",
    "1NNs:0.5902777777777778\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original\n",
    "\n",
    "```\n",
    "regress_group = []\n",
    "k = 0\n",
    "for j in range(pca_d):\n",
    "    X = a_c_train # \n",
    "    y = [vc[j] for vc in v_c_train]\n",
    "    regressor = NuSVR(C=2.0)\n",
    "    #regressor = NuSVR(C=2.0)\n",
    "    regressor.fit(X, y)\n",
    "    regress_group.append(regressor)\n",
    "\n",
    "v_c_test = np.zeros((72, pca_d)) # 提前定义好exemplar矩阵, \n",
    "# 对每一个维度进行预测\n",
    "for j in range(pca_d):\n",
    "    v_c_test[:, j] =  regress_group[j].predict(a_c_test) \n",
    "\n",
    "v_c_test.shape, len(v_c_train), len(v_c_train[0])\n",
    "# also add up to 717 here\n",
    "\n",
    "exem_X, exem_y = [], []\n",
    "\n",
    "for i, c in enumerate(test_class):\n",
    "    exem_X.append(v_c_test[i])\n",
    "    exem_y.append(c)\n",
    "    \n",
    "# Add this part to become GZSL\n",
    "#for i, c in enumerate(train_class):\n",
    "#    exem_X.append(v_c_train[i])\n",
    "#    exem_y.append(c)\n",
    "\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(exem_X, exem_y)\n",
    "print(\"1NN:{} \".format(neigh.score(X_test, y_test)))\n",
    "\n",
    "sneigh = KNeighborsClassifier(n_neighbors=1, metric='seuclidean', \n",
    "                    metric_params={'V':sd_train})\n",
    "sneigh.fit(exem_X, exem_y)\n",
    "print(\"1NNs:{}\".format(sneigh.score(X_test, y_test)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Exemplars and visual features of test class\n",
    "#save_path = \"./sun2_exem/\"\n",
    "#np.save(save_path + \"exem_test.npy\", np.array(exem_X))\n",
    "#np.save(save_path + \"X_test.npy\", X_test)\n",
    "#np.savetxt(save_path + \"y_test.txt\", np.array(y_test, dtype=int), fmt='%s', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proposed Split\n",
    "# Their features\n",
    "## original: 0.6125\n",
    "## MLP variant: 0.5347222222222222\n",
    "# Our features\n",
    "## original: 0.5833333333333334\n",
    "## MLP variant: 0.5041666666666667"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit110dc8947e7b4ce4a04ae067cffb9547"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
