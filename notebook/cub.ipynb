{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import NuSVR, SVR, LinearSVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# different regressor test\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression, ElasticNet\n",
    "\n",
    "import pandas as pd # process txt\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "standard_split: 150 50\nproposed_split: 150 50\n"
    }
   ],
   "source": [
    "folder = \"G:/dataset/cub/CUB_200_2011/\"\n",
    "standard_path = \"G:/dataset/standard_split/CUB/\"\n",
    "proposed_path = \"G:/dataset/proposed_split/CUB/\"\n",
    "\n",
    "cls_to_idx = {}\n",
    "with open(folder + \"classes.txt\", \"r\", encoding='utf-8') as f:\n",
    "     for row in f.readlines():\n",
    "         row = row.rstrip()\n",
    "         idx, name = row.split()\n",
    "         cls_to_idx[name] = int(idx) - 1 # custom will -1\n",
    "\n",
    "sstrain, sstest = [], []\n",
    "pstrain, pstest = [], []\n",
    "\n",
    "# Standard Split \n",
    "with open(standard_path + \"trainvalclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        sstrain.append(cls_to_idx[row])\n",
    "\n",
    "with open(standard_path + \"testclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        sstest.append(cls_to_idx[row])\n",
    "\n",
    "print(\"standard_split:\", len(sstrain), len(sstest))\n",
    "# transform List(str) -> List(int)\n",
    "\n",
    "# Proposed Split\n",
    "with open(proposed_path + \"trainvalclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        pstrain.append(cls_to_idx[row])\n",
    "\n",
    "with open(proposed_path + \"testclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        pstest.append(cls_to_idx[row])\n",
    "\n",
    "print(\"proposed_split:\", len(pstrain), len(pstest))\n",
    "\n",
    "\n",
    "# Random Train & Test Class Split\n",
    "\n",
    "train_class, test_class = [], [] # (5994, 5794)\n",
    "X_class = list(range(200)) # label start from 0, you can adjust it\n",
    "\n",
    "#train_class, test_class = train_test_split(X_class, test_size=0.2)\n",
    "#train_class, test_class = sstrain, sstest\n",
    "\n",
    "train_class, test_class = pstrain, pstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "42, 110, 22, 97, 54, 129, 138, 122, 155, 123, 199, 71, 172, 27, 118, 164, 102, 179, 76, 11, 44, 189, 190, 137, 156, 51, 32, 163, 30, 142, 93, 69, 96, 90, 103, 126, 160, 48, 168, 147, 112, 86, 162, 135, 187, 83, 25, 3, 131, 167\n"
    }
   ],
   "source": [
    "print(*test_class, sep = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[42, 110, 22, 97, 54, 129, 138, 122, 155, 123]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "test_class[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(8821, 2048) (8821,)\n(2967, 2048) (2967,)\n"
    }
   ],
   "source": [
    "class cubRead:\n",
    "    def __init__(self, p, train_split):\n",
    "        \"\"\"\n",
    "        p: 数据集存放路径\n",
    "        train_split: 给出训练集\n",
    "        \"\"\"\n",
    "        X_class = list(range(200)) # custom: range(200)\n",
    "        train_class = train_split\n",
    "        test_class = list(filter(lambda i: i not in train_class, X_class))\n",
    "        self.path = p\n",
    "        # labels\n",
    "        yp = self.path + \"custom-cub-labels.txt\"\n",
    "        y = np.loadtxt(yp, delimiter=\" \", encoding='utf-8')\n",
    "        # visual features 2048 dimensions\n",
    "        xp = self.path + \"custom-cub-features.txt\"\n",
    "        x = np.loadtxt(xp, delimiter=\" \", encoding='utf-8')\n",
    "        i1 = np.isin(y, train_class)\n",
    "        i2 = np.isin(y, test_class)\n",
    "        self.X_train, self.X_test = x[i1], x[i2]\n",
    "        self.y_train, self.y_test = y[i1], y[i2]\n",
    "    def train_data(self):\n",
    "        return self.X_train, self.y_train\n",
    "    def test_data(self):\n",
    "        return self.X_test, self.y_test\n",
    "    def test(self): # 11788 all, add them to test\n",
    "        print(self.X_train.shape, self.y_train.shape)\n",
    "        print(self.X_test.shape, self.y_test.shape)\n",
    "\n",
    "cubReader = cubRead(folder, train_class)\n",
    "cubReader.test() # 8855 + 2933 = 11788\n",
    "# their cub data, 8823 + 2965, 有点问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((8821, 500), (8821,), (2967, 500), (2967,))"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "X_train, y_train = cubReader.train_data()\n",
    "X_test, y_test = cubReader.test_data()\n",
    "#X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "## Hyper Parameters\n",
    "\n",
    "# dimension of PCA\n",
    "\n",
    "pca_d = 500\n",
    "\n",
    "exemPCA = decomposition.PCA(n_components=pca_d)\n",
    "exemPCA.fit(X_train)\n",
    "X_train = exemPCA.transform(X_train)\n",
    "X_test = exemPCA.transform(X_test)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1.28562331e+01, -3.07478366e+00, -2.56507953e+00, ...,\n         2.24887072e-01, -5.02215694e-02,  2.07814415e-02],\n       [ 1.32626356e+01, -2.49965146e+00, -1.92945262e+00, ...,\n        -1.42076074e-01, -2.16375865e-02,  2.32117040e-01],\n       [ 1.36109539e+01, -1.45723334e+00, -1.86439895e+00, ...,\n        -3.48232371e-02,  1.99272638e-01, -1.01119538e-01],\n       ...,\n       [-2.30579039e+00, -2.89017333e+00,  9.98339794e-01, ...,\n        -6.19876985e-02, -2.46070661e-03,  5.60379742e-02],\n       [-2.50973656e-01, -2.99399764e+00,  2.82779538e+00, ...,\n        -1.34863796e-01, -2.36190579e-02,  1.31164547e-01],\n       [ 4.22318316e-01, -5.69984702e+00,  4.40918279e+00, ...,\n        -1.47719562e-02,  3.91515213e-02, -8.82176305e-02]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group up PCA projections\n",
    "\n",
    "exem_train_group = defaultdict(list)\n",
    "\n",
    "for c in train_class:\n",
    "    exem_train_group[c] = []\n",
    "        \n",
    "for x, y in zip(X_train, y_train):\n",
    "    exem_train_group[y].append(x)\n",
    "\n",
    "# Average\n",
    "exem_train = {}\n",
    "std_train = {}\n",
    "# standard deviation\n",
    "\n",
    "k = 0\n",
    "\n",
    "for item in exem_train_group.items():\n",
    "    y, ary = item\n",
    "    exem_train[y] = np.mean(ary, axis=0) # Key Sentence\n",
    "    std_train[y] = np.std(ary, axis=0)\n",
    "\n",
    "del exem_train_group\n",
    "\n",
    "\n",
    "# class to index, the same\n",
    "# 如果是lasso，不能标准化\n",
    "# semantic embedding of CUB, 200 birds, 312 attributes\n",
    "se = folder + \"attributes/class_attribute_labels_continuous.txt\"\n",
    "semat = np.loadtxt(se, delimiter=\" \", encoding='utf-8')\n",
    "\n",
    "#semat = normalize(semat, norm='l2')\n",
    "\n",
    "a_c_train, a_c_test = semat[train_class], semat[test_class]\n",
    "v_c_train = [exem_train[i] for i in train_class] # dict uses classes to index\n",
    "sd_train = [std_train[i] for i in train_class]\n",
    "sd_train = np.mean(sd_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((150, 312), (150, 500), (500,))"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "np.array(a_c_train).shape, np.array(v_c_train).shape, np.array(sd_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(50, 500) 150 500\n1NN:0.47724974721941354 \n1NNs:0.5032018874283788\n"
    }
   ],
   "source": [
    "regress_group = []\n",
    "k = 0\n",
    "for j in range(pca_d):\n",
    "    X = a_c_train # \n",
    "    y = [vc[j] for vc in v_c_train]\n",
    "    regressor = Lasso(alpha=1)\n",
    "    #regressor = NuSVR(C=2)\n",
    "    regressor.fit(X, y)\n",
    "    regress_group.append(regressor)\n",
    "\n",
    "v_c_test = np.zeros((len(test_class), pca_d)) # 提前定义好exemplar矩阵\n",
    "# 对每一个维度进行预测\n",
    "for j in range(pca_d):\n",
    "    v_c_test[:, j] =  regress_group[j].predict(a_c_test) # 10 dimension , assign to column\n",
    "\n",
    "print(v_c_test.shape, len(v_c_train), len(v_c_train[0]))\n",
    "\n",
    "exem_X, exem_y = [], []\n",
    "\n",
    "for i, c in enumerate(test_class):\n",
    "    exem_X.append(v_c_test[i])\n",
    "    exem_y.append(c)\n",
    "\n",
    "\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(exem_X, exem_y)\n",
    "print(\"1NN:{} \".format(neigh.score(X_test, y_test)))\n",
    "\n",
    "sneigh = KNeighborsClassifier(n_neighbors=1, metric='seuclidean', \n",
    "                    metric_params={'V':sd_train})\n",
    "sneigh.fit(exem_X, exem_y)\n",
    "print(\"1NNs:{}\".format(sneigh.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([ 1.86710918,  1.79679195, -0.52144775, -2.05599093, -0.2326713 ,\n         1.57456961, -0.46563282, -0.99361592,  1.68093002,  0.32696533,\n         0.83398897,  1.2802974 ,  0.3472438 , -0.15449661,  1.76555659,\n         1.25386149,  0.93204596,  0.90415291,  2.0464996 , -0.76837355,\n         2.36691703, -4.69707948,  0.12536649, -1.14341364,  0.89576475,\n        -0.14912788,  2.01963394, -0.63166289,  1.85415264,  2.21158489,\n         0.70673272, -1.53107535, -1.85956187,  1.27224217,  0.18085042,\n        -0.18610892,  0.9541327 , -2.53800272,  0.06820689,  0.46773325,\n        -0.22879981,  0.10627698, -1.23894894, -0.62223353, -1.6770004 ,\n         2.05332239, -2.64863689, -2.23173553,  1.17372781,  0.54764598]),\n (50,),\n array([-0.03663805, -0.78066415, -0.93461369,  1.61118045,  0.10878492,\n        -0.94441111, -2.58347483, -0.31397552,  0.43414766, -0.13552903,\n         1.0532155 , -0.91780978,  0.57914565,  0.60233728, -0.48516376,\n        -0.23659848, -0.70632464,  1.06208448, -1.07089871, -2.5245229 ,\n        -2.53117359,  0.11002505, -0.26415069, -1.36522421,  0.52049312,\n         3.88894439,  0.46446891,  0.25337017,  0.27980347, -1.4673658 ,\n         0.7154102 ,  3.30405243, -0.28331778, -1.12579515,  0.44241113,\n        -0.93145077,  0.2370795 , -1.76863372,  0.66158826, -0.40725112,\n        -0.35849372,  5.00072061,  0.96493288, -0.56433267, -1.59727565,\n        -3.23302577, -2.59832434, -2.4710221 , -0.51214292,  1.47699637]))"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "regress_group[4].predict(a_c_test), regress_group[4].predict(a_c_test).shape, regress_group[5].predict(a_c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Exemplars and visual features of test class\n",
    "#save_path = \"./cub_exem/\"\n",
    "#np.save(save_path + \"exem_test.npy\", np.array(exem_X))\n",
    "#np.save(save_path + \"X_test.npy\", X_test)\n",
    "#np.savetxt(save_path + \"y_test.txt\", np.array(y_test, dtype=int), fmt='%s', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = neigh.score(X_test, y_test) deprecated\n",
    "\n",
    "# 4.7 Random Split\n",
    "# ZSL 0.6281342966425839\n",
    "# GZSL 0.23799405014874628\n",
    "\n",
    "# 4.8 Random Split\n",
    "# ZSL 0.6365180467091295\n",
    "\n",
    "# 4.12 Standard Split\n",
    "# ZSL 0.5744971019434026\n",
    "# GZSL 0.18581657006478008\n",
    "\n",
    "# 4.16 Proposed Split\n",
    "# ZSL 0.5466801482979441\n",
    "# MLP variant 0.5284799460734749\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit110dc8947e7b4ce4a04ae067cffb9547"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}