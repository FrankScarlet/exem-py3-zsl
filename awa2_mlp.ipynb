{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import NuSVR # nu-SVR, implement from libsvm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "# different regressor test\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import pandas as pd # process txt\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# 40x85的输入，85x1024的矩阵，变换为40x1024的输出\n",
    "import torch \n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第0步**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard_split: 40 10\n",
      "proposed_split: 40 10\n"
     ]
    }
   ],
   "source": [
    "folder = \"G:/dataset/awa/\"\n",
    "\n",
    "standard_path = \"G:/dataset/standard_split/AWA2/\"\n",
    "proposed_path = \"G:/dataset/proposed_split/AWA2/\" \n",
    "\n",
    "cls_to_idx = {}\n",
    "with open(folder + \"classes.txt\", \"r\", encoding='utf-8') as f:\n",
    "     for row in f.readlines():\n",
    "         row = row.rstrip()\n",
    "         idx, name = row.split()\n",
    "         cls_to_idx[name] = int(idx)\n",
    "\n",
    "sstrain, sstest = [], []\n",
    "pstrain, pstest = [], []\n",
    "\n",
    "with open(standard_path + \"trainvalclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        sstrain.append(cls_to_idx[row])\n",
    "\n",
    "with open(standard_path + \"testclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        sstest.append(cls_to_idx[row])\n",
    "\n",
    "print(\"standard_split:\", len(sstrain), len(sstest))\n",
    "# transform List(str) -> List(int)\n",
    "\n",
    "with open(proposed_path + \"trainvalclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        pstrain.append(cls_to_idx[row])\n",
    "\n",
    "with open(proposed_path + \"testclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        pstest.append(cls_to_idx[row])\n",
    "\n",
    "print(\"proposed_split:\", len(pstrain), len(pstest))\n",
    "\n",
    "# Random Train & Test Class Split\n",
    "X_class = list(range(1, 51))\n",
    "train_class, test_class = train_test_split(X_class, test_size=0.2)\n",
    "\n",
    "train_class, test_class = sstrain, sstest\n",
    "train_class, test_class = pstrain, pstest\n",
    "\n",
    "# labels\n",
    "#yp = folder + \"ResNet101/AwA2-labels.txt\"\n",
    "#y = np.loadtxt(yp, delimiter=\" \", encoding='utf-8')\n",
    "\n",
    "\n",
    "# visual features 2048 dimensions\n",
    "#xp = folder + \"ResNet101/AwA2-features.txt\"\n",
    "#x = np.loadtxt(xp, delimiter=\" \", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 50, 30, 24, 9, 34, 7, 47, 31, 41]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic embedding of AwA2, 85 attibutes\n",
    "seb = folder + \"predicate-matrix-binary.txt\" \n",
    "se = folder + \"predicate-matrix-continuous.txt\" \n",
    "# np.loadtxt(seb, delimiter=\" \", encoding='utf-8'), binary works perfectly\n",
    "# np.loadtxt(se, delimiter=\"  \", encoding='utf-8') # single, double, triple space exists\n",
    "semat = np.zeros((50, 85))\n",
    "with open(se, 'r', encoding='utf-8') as f:\n",
    "    rows = f.readlines()\n",
    "    cnt = 0\n",
    "    for row in rows:\n",
    "        row = row.strip()\n",
    "        semat[cnt, :] = np.array(row.split(), dtype='float64')\n",
    "        cnt = cnt + 1\n",
    "\n",
    "semat.shape # unnormalize\n",
    "semat = preprocessing.normalize(semat) \n",
    "# 标准化后和essay evaluation的一模一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00375358, -0.00375358, -0.00375358, ...,  0.00882092,\n",
       "         0.03640974,  0.03145501],\n",
       "       [ 0.12045618,  0.00426584,  0.        , ...,  0.17996306,\n",
       "         0.0618086 ,  0.03495531],\n",
       "       [ 0.26584459,  0.20652363,  0.        , ...,  0.05026822,\n",
       "         0.04274552,  0.04915256],\n",
       "       ...,\n",
       "       [ 0.22516498,  0.15266022,  0.        , ...,  0.12733492,\n",
       "         0.10009694,  0.01771   ],\n",
       "       [ 0.19613947,  0.1966714 ,  0.        , ...,  0.01787277,\n",
       "         0.06698743,  0.25883601],\n",
       "       [ 0.03819588,  0.08046548,  0.10363715, ...,  0.01479997,\n",
       "         0.05250999,  0.14194515]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29409, 2048) (29409,)\n",
      "(7913, 2048) (7913,)\n"
     ]
    }
   ],
   "source": [
    "# 写成一个类方便其他人使用？\n",
    "# combine x_n and y_n to a class\n",
    "class awaRead:\n",
    "    def __init__(self, p, train_split):\n",
    "        \"\"\"\n",
    "        p: 数据集存放路径\n",
    "        train_split: 给出训练集\n",
    "        \"\"\"\n",
    "        X_class = list(range(1, 51))\n",
    "        train_class = train_split\n",
    "        test_class = list(filter(lambda i: i not in train_class, X_class))\n",
    "        self.path = p\n",
    "        # labels\n",
    "        yp = self.path + \"ResNet101/AwA2-labels.txt\"\n",
    "        y = np.loadtxt(yp, delimiter=\" \", encoding='utf-8')\n",
    "        # visual features 2048 dimensions\n",
    "        xp = self.path + \"ResNet101/AwA2-features.txt\"\n",
    "        x = np.loadtxt(xp, delimiter=\" \", encoding='utf-8')\n",
    "        i1 = np.isin(y, train_class)\n",
    "        i2 = np.isin(y, test_class)\n",
    "        self.X_train, self.X_test = x[i1], x[i2]\n",
    "        self.y_train, self.y_test = y[i1], y[i2]\n",
    "    def train_data(self):\n",
    "        return self.X_train, self.y_train\n",
    "    def test_data(self):\n",
    "        return self.X_test, self.y_test\n",
    "    def test(self):\n",
    "        print(self.X_train.shape, self.y_train.shape)\n",
    "        print(self.X_test.shape, self.y_test.shape)\n",
    "\n",
    "#len(test_class), len(train_class) # (10, 40)\n",
    "awaReader = awaRead(folder, train_class)\n",
    "awaReader.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29409, 500), (29409,), (7913, 500), (7913,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = awaReader.train_data()\n",
    "X_test, y_test = awaReader.test_data()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "# hyper parameter\n",
    "pca_d = 500\n",
    "\n",
    "exemPCA = decomposition.PCA(n_components=pca_d) # 1024 => 500\n",
    "exemPCA.fit(X_train)\n",
    "X_train = exemPCA.transform(X_train)\n",
    "X_test = exemPCA.transform(X_test)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第1步**\n",
    "\n",
    "$$v_c = \\frac {1} {|I_c|} \\sum_{n \\in I_c} M x_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group up PCA projections\n",
    "\n",
    "exem_train_group = defaultdict(list)\n",
    "\n",
    "for c in train_class:\n",
    "    exem_train_group[c] = []\n",
    "        \n",
    "for x, y in zip(X_train, y_train):\n",
    "    exem_train_group[y].append(x)\n",
    "\n",
    "\n",
    "# Average\n",
    "exem_train = {}\n",
    "std_train = {}\n",
    "k = 0\n",
    "\n",
    "for item in exem_train_group.items():\n",
    "    y, ary = item\n",
    "    exem_train[y] = np.mean(ary, axis=0) # Key Sentence\n",
    "    std_train[y] = np.std(ary, axis=0)\n",
    "\n",
    "del exem_train_group\n",
    "\n",
    "trai, tesi = [c-1 for c in train_class], [c-1 for c in test_class] # class to index, start from 0\n",
    "a_c_train, a_c_test = semat[trai], semat[tesi]\n",
    "v_c_train = [exem_train[i] for i in train_class] # dict uses classes to index\n",
    "sd_train = [std_train[i] for i in train_class]\n",
    "sd_train = np.mean(sd_train, axis=0)\n",
    "# exem_train.keys(), train_class to test if they match\n",
    "# e.g. extract the first dimension of exemplar to regression\n",
    "# v_c_i = [vc[0] for vc in v_c_train]\n",
    "# v_c_train[:3], v_c_i[:3] # dimension 0 data\n",
    "# len(v_c_train), len(v_c_train[0]) 40 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 85) (40, 500) (500,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(a_c_train).shape, np.array(v_c_train).shape, np.array(sd_train).shape)\n",
    "###\n",
    "x, y = torch.tensor(np.array(a_c_train), dtype=torch.float), torch.tensor(np.array(v_c_train), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2步 \n",
    "### Training\n",
    "\n",
    "用MLP训练，\n",
    "\n",
    "\n",
    "X为语义表示，是固定的a_c_train矩阵，维度为40x85转换为40x1024\n",
    "\n",
    "$$\\psi (a_c) \\approx v_c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 12925.2529296875\n",
      "449 10871.498046875\n",
      "799 8397.27734375\n",
      "1149 6373.3681640625\n",
      "1499 4843.3095703125\n",
      "1849 3697.82275390625\n",
      "2199 2828.2646484375\n",
      "2549 2151.72998046875\n",
      "2899 1620.2830810546875\n",
      "3249 1205.9376220703125\n",
      "3599 884.9330444335938\n",
      "3949 639.417236328125\n",
      "4299 453.34844970703125\n",
      "4649 313.6085205078125\n",
      "4999 209.99050903320312\n",
      "5349 135.51856994628906\n",
      "5699 84.28010559082031\n",
      "6049 50.39766311645508\n",
      "6399 28.86829948425293\n",
      "6749 15.65655517578125\n"
     ]
    }
   ],
   "source": [
    "# Torch Training\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 40, 85, 300, pca_d\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "#x, y = torch.randn(N, D_in), torch.randn(N, D_out)\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    #torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(7000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 350 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "# 9999 2016.8387451171875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 500) 40 500\n",
      "1NN:0.6258056362947049 \n",
      "1NNs:0.5803108808290155\n"
     ]
    }
   ],
   "source": [
    "#test_x = torch.randn(10, 85)\n",
    "model.eval()\n",
    "test_y = model(torch.tensor(a_c_test, dtype=torch.float))\n",
    "test_y.shape\n",
    "v_c_test = test_y.detach().numpy()\n",
    "# v_c_test = np.zeros((10, 1024)) # 提前定义好exemplar矩阵\n",
    "\n",
    "print(v_c_test.shape, len(v_c_train), len(v_c_train[0]))\n",
    "\n",
    "\n",
    "exem_X, exem_y = [], []\n",
    "\n",
    "for i, c in enumerate(test_class):\n",
    "    exem_X.append(v_c_test[i])\n",
    "    exem_y.append(c)\n",
    "\n",
    "# Add this part to become GZSL\n",
    "#for i, c in enumerate(train_class):\n",
    "#    exem_X.append(v_c_train[i])\n",
    "#    exem_y.append(c)\n",
    "\n",
    "#X_test.shape\n",
    "# exem_X.shape, exem\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(exem_X, exem_y)\n",
    "print(\"1NN:{} \".format(neigh.score(X_test, y_test)))\n",
    "\n",
    "sneigh = KNeighborsClassifier(n_neighbors=1, metric='seuclidean', \n",
    "                    metric_params={'V':sd_train})\n",
    "sneigh.fit(exem_X, exem_y)\n",
    "print(\"1NNs:{}\".format(sneigh.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Log of Training\n",
    "\n",
    "```\n",
    "99 13051.6328125\n",
    "449 11867.4072265625\n",
    "799 10301.38671875\n",
    "1149 9025.2138671875\n",
    "1499 8009.3193359375\n",
    "1849 7143.935546875\n",
    "2199 6734.91357421875\n",
    "2549 6102.67724609375\n",
    "2899 5725.56787109375\n",
    "3249 5473.65673828125\n",
    "3599 5151.4560546875\n",
    "3949 4954.791015625\n",
    "4299 4683.5986328125\n",
    "4649 4459.27197265625\n",
    "4999 4372.97705078125\n",
    "5349 4017.916748046875\n",
    "5699 4114.7744140625\n",
    "6049 3887.33935546875\n",
    "6399 3573.56396484375\n",
    "6749 3627.4140625\n",
    "\n",
    "1NN:0.6766081132313914 \n",
    "1NNs:0.6683937823834197\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3步 alter\n",
    "### Predicting\n",
    "\n",
    "对于a_c_test(10x85)，使用MLP预测得到10x1024的矩阵，这就是我们要求的未见类的视觉特征代表。\n",
    "\n",
    "$\\psi (a_u) = v_u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3步 original\n",
    "### Predicting\n",
    "\n",
    "对于a_c_test(10x85)，依次使用回归器预测出10x1的一个维度的结果，填入结果的一个列中。循环了1024次后得到10x1024的矩阵，这就是我们要求的未见类的视觉特征代表。\n",
    "\n",
    "$\\psi (a_u) = v_u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第4步\n",
    "### Testing\n",
    "突然发现如果是做零样本学习，exem_X就不该加入可见类\n",
    "\n",
    "$\\hat y = arg \\min dis_{NN} (\\psi(M x), v_u) $\n",
    "\n",
    "最近邻分类器，X (10, 1024) y (10, 1)\n",
    "\n",
    "注意v_c_test的顺序，由a_c_test决定，所以根据tesi来分配y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 50, 30, 24, 9, 34, 7, 47, 31, 41]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original\n",
    "\n",
    "```\n",
    "regress_group = []\n",
    "k = 0\n",
    "for j in range(pca_d):\n",
    "    X = a_c_train # \n",
    "    y = [vc[j] for vc in v_c_train]\n",
    "    regressor = NuSVR(C=2)\n",
    "    regressor.fit(X, y)\n",
    "    regress_group.append(regressor)\n",
    "    #print(len(X), len(X[j]), len(y))\n",
    "    #regress_group[j].fit(X, y)\n",
    "\n",
    "# validate\n",
    "# regress_group[10].predict([a_c_train[3]]), v_c_train[3][10]\n",
    "\n",
    "v_c_test = np.zeros((10, pca_d)) # 提前定义好exemplar矩阵\n",
    "# 对每一个维度进行预测\n",
    "for j in range(pca_d):\n",
    "    v_c_test[:, j] =  regress_group[j].predict(a_c_test) # 10 dimension , assign to column\n",
    "\n",
    "v_c_test.shape, len(v_c_train), len(v_c_train[0])\n",
    "\n",
    "\n",
    "exem_X, exem_y = [], []\n",
    "\n",
    "for i, c in enumerate(test_class):\n",
    "    exem_X.append(v_c_test[i])\n",
    "    exem_y.append(c)\n",
    "\n",
    "# Add this part to become GZSL\n",
    "#for i, c in enumerate(train_class):\n",
    "#    exem_X.append(v_c_train[i])\n",
    "#    exem_y.append(c)\n",
    "\n",
    "#X_test.shape\n",
    "# exem_X.shape, exem\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(exem_X, exem_y)\n",
    "print(\"1NN:{} \".format(neigh.score(X_test, y_test)))\n",
    "\n",
    "sneigh = KNeighborsClassifier(n_neighbors=1, metric='seuclidean', \n",
    "                    metric_params={'V':sd_train})\n",
    "sneigh.fit(exem_X, exem_y)\n",
    "print(\"1NNs:{}\".format(sneigh.score(X_test, y_test)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP用于最后的预测\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=30, alpha=1e-4,\n",
    "                    solver='adam', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.01)\n",
    "mlp.fit(exem_X, exem_y)\n",
    "mlp.score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = neigh.score(X_test, y_test)\n",
    "#neigh.predict(X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape, y_test.shape, np.array(exem_X).shape, np.array(exem_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Exemplars and visual features of test class\n",
    "#save_path = \"./awa2_exem/\"\n",
    "#np.save(save_path + \"exem_test.npy\", np.array(exem_X))\n",
    "#np.save(save_path + \"X_test.npy\", X_test)\n",
    "#np.savetxt(save_path + \"y_test.txt\", np.array(y_test, dtype=int), fmt='%s', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP改进\n",
    "\n",
    "MLP用于psi函数的学习，并进行视觉特征代表的预测\n",
    "\n",
    "- 4.17 proposed split:\n",
    "    0.645899153292051 ZSL"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit110dc8947e7b4ce4a04ae067cffb9547"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
