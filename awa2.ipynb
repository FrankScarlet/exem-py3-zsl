{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import NuSVR # nu-SVR, implement from libsvm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "# different regressor test\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge, ElasticNet\n",
    "\n",
    "import pandas as pd # process txt\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. 整理好语义表示 $a_c$，视觉特征 $x_n$\n",
    "1. 创建视觉特征代表：PCA转换后的数据的平均值 $v_c = \\frac {1} {|I_c|} \\sum_{n \\in I_c} M x_n$\n",
    "2. 训练$\\nu$-SVR回归器：$\\psi (a_c) \\approx v_c$，注意是对每一个维度进行预测，理由是PCA将数据的各个维度独立开了。\n",
    "3. 预测未见类的视觉特征代表：$\\psi (a_u) = v_u$\n",
    "4. 新的数据点: $\\hat y = arg \\min dis_{NN} (\\psi(M x), v_u) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第0步**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard_split: 40 10\n",
      "proposed_split: 40 10\n"
     ]
    }
   ],
   "source": [
    "folder = \"G:/dataset/awa/\"\n",
    "\n",
    "standard_path = \"G:/dataset/standard_split/AWA2/\"\n",
    "proposed_path = \"G:/dataset/proposed_split/AWA2/\" \n",
    "\n",
    "cls_to_idx = {}\n",
    "with open(folder + \"classes.txt\", \"r\", encoding='utf-8') as f:\n",
    "     for row in f.readlines():\n",
    "         row = row.rstrip()\n",
    "         idx, name = row.split()\n",
    "         cls_to_idx[name] = int(idx)\n",
    "\n",
    "sstrain, sstest = [], []\n",
    "pstrain, pstest = [], []\n",
    "\n",
    "with open(standard_path + \"trainvalclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        sstrain.append(cls_to_idx[row])\n",
    "\n",
    "with open(standard_path + \"testclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        sstest.append(cls_to_idx[row])\n",
    "\n",
    "print(\"standard_split:\", len(sstrain), len(sstest))\n",
    "# transform List(str) -> List(int)\n",
    "\n",
    "with open(proposed_path + \"trainvalclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        pstrain.append(cls_to_idx[row])\n",
    "\n",
    "with open(proposed_path + \"testclasses.txt\", \"r\", encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        row = row.rstrip()\n",
    "        pstest.append(cls_to_idx[row])\n",
    "\n",
    "print(\"proposed_split:\", len(pstrain), len(pstest))\n",
    "\n",
    "# Random Train & Test Class Split\n",
    "X_class = list(range(1, 51))\n",
    "train_class, test_class = train_test_split(X_class, test_size=0.2)\n",
    "\n",
    "train_class, test_class = sstrain, sstest\n",
    "train_class, test_class = pstrain, pstest\n",
    "\n",
    "# labels\n",
    "#yp = folder + \"ResNet101/AwA2-labels.txt\"\n",
    "#y = np.loadtxt(yp, delimiter=\" \", encoding='utf-8')\n",
    "\n",
    "\n",
    "# visual features 2048 dimensions\n",
    "#xp = folder + \"ResNet101/AwA2-features.txt\"\n",
    "#x = np.loadtxt(xp, delimiter=\" \", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 50, 30, 24, 9, 34, 7, 47, 31, 41]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 85)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# semantic embedding of AwA2, 85 attibutes\n",
    "seb = folder + \"predicate-matrix-binary.txt\" \n",
    "se = folder + \"predicate-matrix-continuous.txt\" \n",
    "# np.loadtxt(seb, delimiter=\" \", encoding='utf-8'), binary works perfectly\n",
    "# np.loadtxt(se, delimiter=\"  \", encoding='utf-8') # single, double, triple space exists\n",
    "semat = np.zeros((50, 85))\n",
    "with open(se, 'r', encoding='utf-8') as f:\n",
    "    rows = f.readlines()\n",
    "    cnt = 0\n",
    "    for row in rows:\n",
    "        row = row.strip()\n",
    "        semat[cnt, :] = np.array(row.split(), dtype='float64')\n",
    "        cnt = cnt + 1\n",
    "\n",
    "semat.shape # unnormalize\n",
    "# semat = preprocessing.normalize(semat) \n",
    "# 标准化后和essay evaluation的一模一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29409, 2048) (29409,)\n",
      "(7913, 2048) (7913,)\n"
     ]
    }
   ],
   "source": [
    "# 写成一个类方便其他人使用？\n",
    "# combine x_n and y_n to a class\n",
    "class awaRead:\n",
    "    def __init__(self, p, train_split):\n",
    "        \"\"\"\n",
    "        p: 数据集存放路径\n",
    "        train_split: 给出训练集\n",
    "        \"\"\"\n",
    "        X_class = list(range(1, 51))\n",
    "        train_class = train_split\n",
    "        test_class = list(filter(lambda i: i not in train_class, X_class))\n",
    "        self.path = p\n",
    "        # labels\n",
    "        yp = self.path + \"ResNet101/AwA2-labels.txt\"\n",
    "        y = np.loadtxt(yp, delimiter=\" \", encoding='utf-8')\n",
    "        # visual features 2048 dimensions\n",
    "        xp = self.path + \"ResNet101/AwA2-features.txt\"\n",
    "        x = np.loadtxt(xp, delimiter=\" \", encoding='utf-8')\n",
    "        i1 = np.isin(y, train_class)\n",
    "        i2 = np.isin(y, test_class)\n",
    "        self.X_train, self.X_test = x[i1], x[i2]\n",
    "        self.y_train, self.y_test = y[i1], y[i2]\n",
    "    def train_data(self):\n",
    "        return self.X_train, self.y_train\n",
    "    def test_data(self):\n",
    "        return self.X_test, self.y_test\n",
    "    def test(self):\n",
    "        print(self.X_train.shape, self.y_train.shape)\n",
    "        print(self.X_test.shape, self.y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "#len(test_class), len(train_class) # (10, 40)\n",
    "awaReader = awaRead(folder, train_class)\n",
    "awaReader.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = awaReader.train_data()\n",
    "X_test, y_test = awaReader.test_data()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "## Hyper Parameters\n",
    "\n",
    "# dimension of PCA\n",
    "\n",
    "pca_d = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29409, 500), (29409,), (7913, 500), (7913,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemPCA = decomposition.PCA(n_components=pca_d) # 1024 => 500\n",
    "exemPCA.fit(X_train)\n",
    "X_train = exemPCA.transform(X_train)\n",
    "X_test = exemPCA.transform(X_test)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第1步**\n",
    "\n",
    "$$v_c = \\frac {1} {|I_c|} \\sum_{n \\in I_c} M x_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group up PCA projections\n",
    "\n",
    "exem_train_group = defaultdict(list)\n",
    "\n",
    "for c in train_class:\n",
    "    exem_train_group[c] = []\n",
    "        \n",
    "for x, y in zip(X_train, y_train):\n",
    "    exem_train_group[y].append(x)\n",
    "\n",
    "\n",
    "# Average\n",
    "exem_train = {}\n",
    "std_train = {}\n",
    "k = 0\n",
    "\n",
    "for item in exem_train_group.items():\n",
    "    y, ary = item\n",
    "    exem_train[y] = np.mean(ary, axis=0) # Key Sentence\n",
    "    std_train[y] = np.std(ary, axis=0)\n",
    "\n",
    "del exem_train_group\n",
    "\n",
    "trai, tesi = [c-1 for c in train_class], [c-1 for c in test_class] # class to index, start from 0\n",
    "a_c_train, a_c_test = semat[trai], semat[tesi]\n",
    "v_c_train = [exem_train[i] for i in train_class] # dict uses classes to index\n",
    "sd_train = [std_train[i] for i in train_class]\n",
    "sd_train = np.mean(sd_train, axis=0)\n",
    "# exem_train.keys(), train_class to test if they match\n",
    "# e.g. extract the first dimension of exemplar to regression\n",
    "# v_c_i = [vc[0] for vc in v_c_train]\n",
    "# v_c_train[:3], v_c_i[:3] # dimension 0 data\n",
    "# len(v_c_train), len(v_c_train[0]) 40 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 85), (40, 500), (500,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a_c_train).shape, np.array(v_c_train).shape, np.array(sd_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2步 \n",
    "### Training\n",
    "$\\nu$-SVR，注意是对每一个维度。比如说1024个维度，就是用1024个回归器通过语义表示预测每一个维度的数据。\n",
    "\n",
    "\n",
    "X为语义表示，是固定的a_c_train矩阵，维度为40x85。比如对应第0维的视觉数据y为40x1（对于40x1024），这样就相当于给第一个回归器投入了40个训练数据，一直循环到1024维。\n",
    "\n",
    "$$\\psi (a_c) \\approx v_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3步\n",
    "### Predicting\n",
    "\n",
    "对于a_c_test(10x85)，依次使用回归器预测出10x1的一个维度的结果，填入结果的一个列中。循环了1024次后得到10x1024的矩阵，这就是我们要求的未见类的视觉特征代表。\n",
    "\n",
    "$\\psi (a_u) = v_u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第4步\n",
    "### Testing\n",
    "突然发现如果是做零样本学习，exem_X就不该加入可见类\n",
    "\n",
    "$\\hat y = arg \\min dis_{NN} (\\psi(M x), v_u) $\n",
    "\n",
    "最近邻分类器，X (10, 1024) y (10, 1)\n",
    "\n",
    "注意v_c_test的顺序，由a_c_test决定，所以根据tesi来分配y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 500) 40 500\n",
      "1NN:0.6515859977252623 \n",
      "1NNs:0.6333880955389865\n"
     ]
    }
   ],
   "source": [
    "regress_group = []\n",
    "k = 0\n",
    "for j in range(pca_d):\n",
    "    X = a_c_train # \n",
    "    y = [vc[j] for vc in v_c_train]\n",
    "    regressor = Lasso()\n",
    "    #regressor = NuSVR(C=2)\n",
    "    regressor.fit(X, y)\n",
    "    regress_group.append(regressor)\n",
    "\n",
    "# validate\n",
    "# regress_group[10].predict([a_c_train[3]]), v_c_train[3][10]\n",
    "# regress_group[4].predict(a_c_test), regress_group[5].predict(a_c_test)\n",
    "\n",
    "v_c_test = np.zeros((10, pca_d)) # 提前定义好exemplar矩阵\n",
    "# 对每一个维度进行预测\n",
    "for j in range(pca_d):\n",
    "    v_c_test[:, j] =  regress_group[j].predict(a_c_test) # 10 dimension , assign to column\n",
    "\n",
    "print(v_c_test.shape, len(v_c_train), len(v_c_train[0]))\n",
    "\n",
    "exem_X, exem_y = [], []\n",
    "\n",
    "for i, c in enumerate(test_class):\n",
    "    exem_X.append(v_c_test[i])\n",
    "    exem_y.append(c)\n",
    "\n",
    "# Add this part to become GZSL\n",
    "#for i, c in enumerate(train_class):\n",
    "#    exem_X.append(v_c_train[i])\n",
    "#    exem_y.append(c)\n",
    "\n",
    "#X_test.shape\n",
    "# exem_X.shape, exem\n",
    "\n",
    "#exem_X, exem_y = [], []\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(exem_X, exem_y)\n",
    "print(\"1NN:{} \".format(neigh.score(X_test, y_test)))\n",
    "\n",
    "sneigh = KNeighborsClassifier(n_neighbors=1, metric='seuclidean', \n",
    "                    metric_params={'V':sd_train})\n",
    "sneigh.fit(exem_X, exem_y)\n",
    "print(\"1NNs:{}\".format(sneigh.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.,  0.,\n",
       "        0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0.,  0.,\n",
       "        0., -0., -0., -0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0., -0.,\n",
       "        0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0., -0., -0.,  0., -0.,\n",
       "       -0.,  0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0.,  0., -0.,\n",
       "       -0.,  0.,  0.,  0.,  0., -0.,  0.,  0., -0., -0., -0.,  0., -0.,\n",
       "       -0.,  0.,  0.,  0., -0., -0.,  0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress_group[299].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 50, 30, 24, 9, 34, 7, 47, 31, 41]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test[:2], y_test[:2], v_c_test[6], v_c_test[6].shape\n",
    "#neigh.predict(X_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = neigh.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy \n",
    "# \n",
    "#  4/3 Random Splt\n",
    "#  0.6214436868093111 ZSL\n",
    "#  0.26980954620268044 GZSL\n",
    "# 4/12 Standard Split\n",
    "# 0.658267716535433 ZSL\n",
    "# 0.10966356478167502 GZSL \n",
    "# 4/13 Proposed Split\n",
    "# 0.3626943005181347 ZSL\n",
    "# 0.1228358397573613 GZSL\n",
    "# 4/14 Standard Split with original continuous semantic embedding \n",
    "# 0.7448818897637796 ZSL\n",
    "# 0.14373657838224768 GZSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape, y_test.shape, np.array(exem_X).shape, np.array(exem_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Exemplars and visual features of test class\n",
    "save_path = \"./lasso_exem/\"\n",
    "np.save(save_path + \"exem_test.npy\", np.array(exem_X))\n",
    "np.save(save_path + \"X_test.npy\", X_test)\n",
    "np.savetxt(save_path + \"y_test.txt\", np.array(y_test, dtype=int), fmt='%s', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验结果\n",
    "\n",
    "> 最初的结果都使用了binary的语义表示，原版给了一个未标准化的，\n",
    "\n",
    "- 4/3 Random Splt C=2\n",
    "\n",
    "0.6214436868093111 ZSL,\n",
    "0.26980954620268044 GZSL\n",
    "\n",
    "- 4/12 Standard Split C=2\n",
    "\n",
    "0.658267716535433 ZSL,\n",
    "0.10966356478167502 GZSL \n",
    "\n",
    "- 4/13 Proposed Split C=2\n",
    "\n",
    "0.3626943005181347 ZSL,\n",
    "0.1228358397573613 GZSL\n",
    "\n",
    "- 4/14 Standard Split with original continuous semantic embedding\n",
    "    - C = default|| 0.7448818897637796 ZSL\n",
    "    - C = 2 || 0.7730851825340014 ZSL\n",
    "    try to use different regressor:\n",
    "    - GradientBoostingRegressor 0.6641374373657838 \n",
    "    - SVR 0.7589119541875448\n",
    "    - LinearSVR 0.6943450250536864\n",
    "    - LinearRegression 0.697494631352899\n",
    "    - Lasso 0.7156764495347172\n",
    "    - Ridge 0.697780959198282\n",
    "\n",
    "- 4/16 Proposed Split with continuous semantic embedding\n",
    "    - C = 2 || 0.5937065588272463 ZSL\n",
    "    - C = defalut || 0.5348161253633261 ZSL\n",
    "    - SVR || 0.4821180336155693 ZSL"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit110dc8947e7b4ce4a04ae067cffb9547"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
